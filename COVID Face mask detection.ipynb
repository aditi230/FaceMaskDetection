{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pwl7X8n1cf96"
   },
   "outputs": [],
   "source": [
    "label_dict={\"with_mask\": 0,\"without_mask\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOdf12373XLJ"
   },
   "outputs": [],
   "source": [
    "categories=[\"with_mask\",\"without_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3Tkk9bP3ik-"
   },
   "outputs": [],
   "source": [
    "labels=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ev3meTY8Zh_"
   },
   "outputs": [],
   "source": [
    "data_path=\"D:\\\\Verzeo\\\\Facemask\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToQmpm7w9QZO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNeOi8_E7HmJ"
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "target=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "nFfUulma8G5l",
    "outputId": "0c5fb592-de55-4ef0-99c9-254aa2c97e62"
   },
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        try: \n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            resized=cv2.resize(gray,(100,100))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjSDu4mCANLq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNXw4kC2A2Kk"
   },
   "outputs": [],
   "source": [
    "data=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nz417AXoD14V"
   },
   "outputs": [],
   "source": [
    "data=data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.reshape(data,(data.shape[0],100,100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 100, 100, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target=np_utils.to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(200,(3,3),input_shape=data.shape[1:],activation=\"relu\")) #it helps us to detect edges\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) #reduce the number of pixel by keeping max pixel value\n",
    "model.add(Conv2D(100,(3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_target,test_target=train_test_split(data,new_target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.5343WARNING:tensorflow:From C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "31/31 [==============================] - 31s 986ms/step - loss: 0.7037 - accuracy: 0.5343 - val_loss: 0.6757 - val_accuracy: 0.5927\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.7444INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "31/31 [==============================] - 31s 999ms/step - loss: 0.5187 - accuracy: 0.7444 - val_loss: 0.3592 - val_accuracy: 0.8669\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8414INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "31/31 [==============================] - 31s 996ms/step - loss: 0.3491 - accuracy: 0.8414 - val_loss: 0.2776 - val_accuracy: 0.8911\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9131INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "31/31 [==============================] - 31s 998ms/step - loss: 0.2343 - accuracy: 0.9131 - val_loss: 0.2689 - val_accuracy: 0.8911\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9404INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "31/31 [==============================] - 31s 998ms/step - loss: 0.1711 - accuracy: 0.9404 - val_loss: 0.2460 - val_accuracy: 0.9315\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9525INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "31/31 [==============================] - 31s 999ms/step - loss: 0.1346 - accuracy: 0.9525 - val_loss: 0.1797 - val_accuracy: 0.9315\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9707INFO:tensorflow:Assets written to: model-007.model\\assets\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0842 - accuracy: 0.9707 - val_loss: 0.1721 - val_accuracy: 0.9476\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9768INFO:tensorflow:Assets written to: model-008.model\\assets\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0683 - accuracy: 0.9768 - val_loss: 0.1617 - val_accuracy: 0.9476\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.0695 - accuracy: 0.9707 - val_loss: 0.1830 - val_accuracy: 0.9476\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0582 - accuracy: 0.9859 - val_loss: 0.1837 - val_accuracy: 0.9395\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 31s 987ms/step - loss: 0.0384 - accuracy: 0.9859 - val_loss: 0.1861 - val_accuracy: 0.9435\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 30s 967ms/step - loss: 0.0344 - accuracy: 0.9899 - val_loss: 0.1786 - val_accuracy: 0.9556\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 30s 971ms/step - loss: 0.0407 - accuracy: 0.9828 - val_loss: 0.2029 - val_accuracy: 0.9395\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 30s 965ms/step - loss: 0.0376 - accuracy: 0.9869 - val_loss: 0.1790 - val_accuracy: 0.9395\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 30s 962ms/step - loss: 0.0215 - accuracy: 0.9949 - val_loss: 0.2851 - val_accuracy: 0.9274\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 30s 966ms/step - loss: 0.0507 - accuracy: 0.9828 - val_loss: 0.2052 - val_accuracy: 0.9516\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 30s 975ms/step - loss: 0.0190 - accuracy: 0.9960 - val_loss: 0.1682 - val_accuracy: 0.9435\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 30s 961ms/step - loss: 0.0214 - accuracy: 0.9909 - val_loss: 0.2650 - val_accuracy: 0.9274\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 30s 978ms/step - loss: 0.0276 - accuracy: 0.9909 - val_loss: 0.1734 - val_accuracy: 0.9435\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 31s 985ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.1988 - val_accuracy: 0.9476\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 31s 990ms/step - loss: 0.0258 - accuracy: 0.9899 - val_loss: 0.1698 - val_accuracy: 0.9597\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 30s 970ms/step - loss: 0.0154 - accuracy: 0.9960 - val_loss: 0.2209 - val_accuracy: 0.9637\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 30s 983ms/step - loss: 0.0206 - accuracy: 0.9919 - val_loss: 0.2062 - val_accuracy: 0.9355\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 30s 975ms/step - loss: 0.0291 - accuracy: 0.9909 - val_loss: 0.1664 - val_accuracy: 0.9476\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 30s 965ms/step - loss: 0.0305 - accuracy: 0.9889 - val_loss: 0.2231 - val_accuracy: 0.9355\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 30s 961ms/step - loss: 0.0275 - accuracy: 0.9899 - val_loss: 0.1835 - val_accuracy: 0.9435\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 30s 980ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 0.1631 - val_accuracy: 0.9597\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 30s 961ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 0.2286 - val_accuracy: 0.9355\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9990INFO:tensorflow:Assets written to: model-029.model\\assets\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.1556 - val_accuracy: 0.9597\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 30s 962ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.1675 - val_accuracy: 0.9516\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 30s 962ms/step - loss: 0.0283 - accuracy: 0.9889 - val_loss: 0.1991 - val_accuracy: 0.9435\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 30s 962ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.1705 - val_accuracy: 0.9516\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 30s 966ms/step - loss: 0.0059 - accuracy: 0.9970 - val_loss: 0.1677 - val_accuracy: 0.9597\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 30s 964ms/step - loss: 0.0117 - accuracy: 0.9949 - val_loss: 0.2222 - val_accuracy: 0.9395\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 31s 985ms/step - loss: 0.0064 - accuracy: 0.9970 - val_loss: 0.2274 - val_accuracy: 0.9476\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 31s 987ms/step - loss: 0.0170 - accuracy: 0.9960 - val_loss: 0.1719 - val_accuracy: 0.9597\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 30s 973ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.2003 - val_accuracy: 0.9476\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 31s 985ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.2025 - val_accuracy: 0.9516\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 31s 988ms/step - loss: 0.0089 - accuracy: 0.9960 - val_loss: 0.2019 - val_accuracy: 0.9677\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 31s 987ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 0.1900 - val_accuracy: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1480bb4a400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint(\"model-{epoch:03d}.model\",save_best_only=True,mode=\"auto\")\n",
    "model.fit(train_data,train_target,epochs=40,validation_split=0.2,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_image=\"D:\\\\test\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "timg_names=os.listdir(t_image)\n",
    "for timg_name in timg_names:\n",
    "    timg_path=os.path.join(t_image,timg_name)\n",
    "    timg=cv2.imread(timg_path)\n",
    "    try:\n",
    "        tgray=cv2.cvtColor(timg,cv2.COLOR_BGR2GRAY)\n",
    "        tresized=cv2.resize(tgray,(100,100))\n",
    "        test.append(tresized)\n",
    "      \n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.reshape(test,(test.shape[0],100,100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model=tf.keras.Sequential([model,tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=probability_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 image is without_mask\n",
      "2 image is with_mask\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i in prediction:\n",
    "    lab=np.argmax(i)\n",
    "    c=c+1\n",
    "    print(c,\"image is\",categories[lab])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab=np.argmax(prediction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml file\n",
    "face_cascader=cv2.CascadeClassifier(\"D:\\\\Verzeo\\\\14th Lecture-20200908T074323Z-001\\\\14th Lecture\\\\haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specific image\n",
    "img=cv2.imread(\"D:\\\\test\\\\mask.jpg\")\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "faces=face_cascader.detectMultiScale(gray)\n",
    "for(x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv2.rectangle(img,(x,y-40),(x+w,y),(0,0,255),2)\n",
    "    cv2.putText(img,categories[lab],(x+2,y-10),cv2.FONT_HERSHEY_SIMPLEX,1.0,(255,0,0),2)\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b0b62488bc7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_cascader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#videoCapture\n",
    "cap=cv2.VideoCapture(\"D:\\\\test\\\\face.mp4\") #0 to capture webcame\n",
    "while True:\n",
    "    _,img=cap.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascader.detectMultiScale(gray)\n",
    "    for(x,y,w,h) in faces:\n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        \n",
    "        normimage=resized/255\n",
    "        reshapeimage=np.reshape(normimage,(-1,100,100,1))\n",
    "        modelop=model.predict(reshapeimage)\n",
    "        \n",
    "        label=np.argmax(modelop,axis=1)[1]\n",
    "      \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],1)\n",
    "        \n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "       \n",
    "        \n",
    "    cv2.imshow(\"checking...\",img)\n",
    "    key=cv2.waitKey(2)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webcam\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "source=cv2.VideoCapture(0)\n",
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    \n",
    "    faces=face_cascader.detectMultiScale(img,1.3,5)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        \n",
    "        \n",
    "        \n",
    "        normimage=resized/255\n",
    "        reshapeimage=np.reshape(normimage,(-1,100,100,1))\n",
    "        modelop=model.predict(reshapeimage)\n",
    "        \n",
    "        label=np.argmax(modelop,axis=1)[1]\n",
    "      \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],1)\n",
    "        \n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "       \n",
    "        \n",
    "    cv2.imshow(\"checking...\",img)\n",
    "    key=cv2.waitKey(2)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COVID.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
